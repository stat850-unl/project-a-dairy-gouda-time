---
title: "Wikiscrape"
author: "Becca Furbeck"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First of all...the FDA makes getting their data somewhat irritating to me (as in it does not come in comma-separated values). XML scrape it is! I procured data from the last 5 years.
```{r}
#2020-2018 are on current listing exportable as excel from a jquery table that didn't want scraped...it goes into git repo as FDA.csv it that case
FDAcurrentsite="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts"
FDA1<-read.csv("FDA.csv", skip = 1)
```

```{r}
library(XML)
library(dplyr)
#Archived 2017 has 14 pages
FDA2017site='http://wayback.archive-it.org/7993/20180125100707/https://www.fda.gov/Safety/Recalls/ArchiveRecalls/2017/default.htm?Page=14'
#Archived 2016 has 18 pages
FDA2016site = 'http://wayback.archive-it.org/7993/20180125100804/https://www.fda.gov/Safety/Recalls/ArchiveRecalls/2016/default.htm?Page=17'

tbls_xml <- readHTMLTable(FDA2017site)

tbls16<-capture.output(for (i in 1:14){
  print(paste0('http://wayback.archive-it.org/7993/20180125100707/https://www.fda.gov/Safety/Recalls/ArchiveRecalls/2017/default.htm?Page=',i,''))
})

tbls17<-capture.output(for (i in 1:18){
  (print(paste0('http://wayback.archive-it.org/7993/20180125100804/https://www.fda.gov/Safety/Recalls/ArchiveRecalls/2016/default.htm?Page=',i,'')))
})

FDAoldtbl<-c(tbls16,tbls17)
FDAoldtabl<-as.data.frame(FDAoldtbl)
front<-gsub("^.....","",FDAoldtabl$FDAoldtbl)
back<-gsub('.{1}$','', front)
OldJoin<-back %>% map(readHTMLTable)

Old<-rbind(OldJoin[[1]][["NULL"]],OldJoin[[2]][["NULL"]],OldJoin[[3]][["NULL"]],OldJoin[[4]][["NULL"]],OldJoin[[5]][["NULL"]],OldJoin[[6]][["NULL"]],OldJoin[[7]][["NULL"]],OldJoin[[8]][["NULL"]],OldJoin[[9]][["NULL"]],OldJoin[[10]][["NULL"]],OldJoin[[11]][["NULL"]],OldJoin[[12]][["NULL"]],OldJoin[[13]][["NULL"]],OldJoin[[14]][["NULL"]],OldJoin[[15]][["NULL"]],OldJoin[[16]][["NULL"]],OldJoin[[17]][["NULL"]],OldJoin[[18]][["NULL"]],OldJoin[[19]][["NULL"]],OldJoin[[20]][["NULL"]],OldJoin[[21]][["NULL"]],OldJoin[[22]][["NULL"]],OldJoin[[23]][["NULL"]],OldJoin[[24]][["NULL"]],OldJoin[[25]][["NULL"]],OldJoin[[26]][["NULL"]],OldJoin[[27]][["NULL"]],OldJoin[[28]][["NULL"]],OldJoin[[29]][["NULL"]],OldJoin[[30]][["NULL"]],OldJoin[[31]][["NULL"]],OldJoin[[32]][["NULL"]])
Old<-as.data.frame(Old)


Old<-Old %>% select(-" Details/Photo ") 
FDA1<-FDA1 %>% select(-"Product.Type")

matchnames<-c("Date","Brand.Name.s.","Product.Description","Recall.Reason.Description","Company.Name")
colnames(Old)<-matchnames
Old<-as.data.frame(Old)
FDA1<-as.data.frame(FDA1)
FDA<-rbind(Old,FDA1)
```


Well...the FDA recall dataset is actually a mess. One of the items they recalled was Whole Foods 8-20-20. What does that mean? An entire store? Also, sometimes items are put into weird categories, so I decided to search the dairy items by the Production Description.
```{r FDA}
library(dplyr)
FDA %>% filter(Product.Description=="8-20-2020, Whole Foods Market")
```

I had to find a good list of dairy products that would be current and appropriate. The federal register has a list for U.S. specific items, but I wanted a broader worldview. Wikipedia has lists of dairy products and cheeses, so I figured if I extracted the items from the pages, I could use these to search the FDA database. Also, I just wanted to try scraping html tables. 
```{r wikipedia}
library(purrr)
library(htmltab)
library(tidyr)
url = 'https://en.wikipedia.org/wiki/List_of_dairy_products'
url2 = "https://en.wikipedia.org/wiki/List_of_cheeses"

tbls <- map2(url, 1:23, htmltab, rm_nodata_cols =F)
tblname<-map(tbls, 1)
dairy1<-unlist(tblname)

tbls2 <- map2(url2, 1:23, htmltab, rm_nodata_cols =F)
tblname2<-map(tbls2, 1)
dairy2<-unlist(tblname2)

dairy<-c(dairy1,dairy2)
```


Then the filtering nightmare. "So" pulled soybean so I had to pull that out, and "butter" gave an inevitable "Nut butter" problem that had to be negated. 
```{r FDA filter}
library(stringr)
library(dplyr)

dairy
dairy2<-print(dairy[dairy!="So"])     

matchesdairy <- grepl(paste(dairy2, collapse = "|"), FDA$Product.Description)
matchesdairy2<-print(FDA$Product.Description[matchesdairy])

getout<-c("Butter nut","Butternut", "Peanut", "Non-Dairy","Butterfly", "Beanit","Panque","Sunflower","Nut", "Cashew", "Imitation")

filtereditems <- grepl(paste(getout, collapse = "|"), matchesdairy2)
filtereditems2<-print(matchesdairy2[filtereditems==F])

DairySet<-FDA %>% filter(Product.Description %in% filtereditems2)

```

Now that we have filtered the relevant items, it is time to investigate what are the commons reasons for recall. In order to make a visualization for this, I made a wordcloud. 
```{r}
library(stringr)
library(tm)
library(RColorBrewer)
library(wordcloud)

nopunct<-gsub('[[:punct:] ]+',' ',DairySet$Recall.Reason.Description)
nopunct<-trimws(nopunct)
wordsextract<-str_split(nopunct, " ")
wordsextract<-unlist(wordsextract)

docs <- Corpus(VectorSource(wordsextract))
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
wordclouddf <- data.frame(word = names(words),freq=words)


set.seed(5658) # for reproducibility 
wordcloud(words = wordclouddf$word, freq = wordclouddf$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.20,colors=brewer.pal(8, "PuOr"))
#purple and orange to be color friendly
```